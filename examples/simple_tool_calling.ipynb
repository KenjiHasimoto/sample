{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "sys.path.append('../../aisuite')\n",
    "\n",
    "# Load from .env file if available\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock tool functions.\n",
    "def get_current_temperature(location: str, unit: str):\n",
    "    # Simulate fetching temperature from an API\n",
    "    return {\"location\": location, \"unit\": unit, \"temperature\": 72}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_raining(location: str):\n",
    "    \"\"\"Check if it is raining at a location.\n",
    "\n",
    "    Args:\n",
    "        location (str): Name of the Place.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether it is raining in that place.\n",
    "    \"\"\"\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"anthropic:claude-3-5-sonnet-20240620\"\n",
    "# model = \"openai:gpt-4o\"\n",
    "# model = mistral:mistral-large-latest\n",
    "# model = \"aws:anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# model = \"aws:meta.llama3-1-8b-instruct-v1:0\"\n",
    "# model = \"aws:meta.llama3-3-70b-instruct-v1:0\"\n",
    "# model = \"groq:llama-3.1-70b-versatile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisuite import Client, ToolManager  # Import your ToolManager class\n",
    "\n",
    "client = Client()\n",
    "tool_manager = ToolManager([get_current_temperature, is_it_raining])\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the current temperature in San Francisco in Celsius?\"}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tool_manager.tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'pprint.pprint(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable. Did you mean: 'pprint.pprint(...)'?"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the function result, the current temperature in San Francisco is 72 degrees Celsius.\n",
      "\n",
      "However, I must point out that this temperature seems unusually high for San Francisco, especially in Celsius. A temperature of 72°C would be equivalent to about 161.6°F, which is extremely hot and not typical for San Francisco's climate. \n",
      "\n",
      "It's possible there might be an error in the data or in how the function is interpreting or reporting the temperature. In a normal situation, we would expect San Francisco's temperature to be much lower, typically between 10°C to 25°C (50°F to 77°F) depending on the time of year.\n",
      "\n",
      "If you'd like, we can double-check this information or try to get the temperature in Fahrenheit to compare. Would you like me to do that?\n"
     ]
    }
   ],
   "source": [
    "if response.choices[0].message.tool_calls:\n",
    "    tool_results, result_as_message = tool_manager.execute_tool(response.choices[0].message.tool_calls)\n",
    "    messages.append(response.choices[0].message) # Model's function call message\n",
    "    messages.append(result_as_message[0])\n",
    "\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=model, messages=messages, tools=tool_manager.tools())\n",
    "    print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, test without tool calling, to check that the normal path is not broken.\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of California?\"}]\n",
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
